{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: Develop algorithm to find redundancy and synergy by characterizing group invariant enputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cana.boolean_node import BooleanNode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Test Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 000 001 010 011 100 101 110 111 ECA rule 151\n",
    "#  1   1   1   0   1   0   0   1\n",
    "# make cana BooleanNode for calulations later\n",
    "outputs = ['1', '1', '1', '0', '1', '0', '0', '1']\n",
    "gate = BooleanNode.from_output_list(outputs)\n",
    "from wand.image import Image\n",
    "Image(filename='../plots/example_luts_schemata/high_synergy/parity_and_off_lut.pdf')\n",
    "\n",
    "# Here is another test gate that doesn't nicely reduce into a three-way permutation\n",
    "# rule 190\n",
    "outputs = ['0', '1', '1', '1', '1', '1', '0', '1'] \n",
    "gate = BooleanNode.from_output_list(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something I said in my little writeup is that the schemata $f''_3$ shows a redundancy between synergies of all possible pairs. The algorithm I am developing here should be able to find that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to go through each subset in the powerset and see whether or not the inputs in the subset make up a group invarient enput in the two symbol schemata. I think we want to be very inclusive here and count subsets with cardinality 1 as \"group invarient enputs\" although I'm not there yet. We also want to count all inputs sharing the same literal as in the 000 -> 1 or 111 -> 1 transitions shown in the look up table above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('000', [], [[0, 1, 2]]), ('110', [], [[0, 1]])]\n[('012', [[0, 1]], []), ('221', [], [[0, 1]])]\n"
    }
   ],
   "source": [
    "# calculate the two symbol schemata for the logic gate\n",
    "gate.input_symmetry()\n",
    "ts0s, ts1s = gate._two_symbols\n",
    "pi0s, pi1s = gate._prime_implicants\n",
    "print(ts0s)\n",
    "print(ts1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The organization here is a little weird and we're going to need to clean it up a bit. The entries here are in the following format:\n",
    "\n",
    "    (reduced input entry (2 means #), list of permutable groups with different input states, list of permutable groups who share input states)\n",
    "\n",
    "We don't care about the difference between permutable groups with the same symbols or different symbols. They are all symbols. We do want to preserve information about the transition, that is crucial to localizing the information.\n",
    "\n",
    "We will populate a new list of permutable inputs using only the supersets of the permutable inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('0', '000', [{0, 1, 2}]),\n ('0', '110', [{0, 1}]),\n ('1', '012', [{0, 1}]),\n ('1', '221', [{0, 1}])]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "ts_transitions = []\n",
    "# iterate over the schemata and the output labels so we can easuly preserve that information\n",
    "for output, ts in zip(['0', '1'], [ts0s, ts1s]):\n",
    "    for inputs, permutables, same_symbols in ts:\n",
    "        # big groups will be the ones we keep, all groups is all possibilities that cana finds\n",
    "        big_groups = []\n",
    "        all_groups = permutables + same_symbols\n",
    "        all_groups = [set(group) for group in all_groups]\n",
    "        # we need to eliminate any group that is a subset of any other\n",
    "        # this is a probably pretty bad way to do that\n",
    "        if len(all_groups) <= 1:\n",
    "            big_groups = all_groups\n",
    "        else:\n",
    "            for group in all_groups:\n",
    "                for check_group in all_groups:\n",
    "                    if group > check_group and group not in big_groups:\n",
    "                        big_groups.append(group)\n",
    "        # make a new entry for the table thing\n",
    "        new_entry = (output, inputs, big_groups)\n",
    "        ts_transitions.append(new_entry)\n",
    "ts_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else we're going to need is a function to get the powerset. Heres one from itertools documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0,), (1,), (2,), (0, 1), (0, 2), (1, 2), (0, 1, 2)]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# modified from itertools documentation\n",
    "from itertools import chain, combinations\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))\n",
    "list(powerset(range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts_coverage doesn't seem to work but we can unpack the coverage from the two symbol schemata and calculate their coverage directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('0', '000', [{0, 1, 2}], 1),\n ('0', '110', [{0, 1}], 1),\n ('1', '012', [{0, 1}], 2),\n ('1', '221', [{0, 1}], 4)]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "def expand_wildcards(state_list):\n",
    "    \"\"\" recursively replaces wildcards in a set of input states \"\"\"\n",
    "    expanded = set([])\n",
    "    for i, states in enumerate(state_list):\n",
    "        states = tuple(states)\n",
    "        if '2' not in states:\n",
    "            expanded.add(states)\n",
    "        else:\n",
    "            for j, char in enumerate(states):\n",
    "                if char == '2':\n",
    "                    gp_ones = list(states)\n",
    "                    gp_ones[j] = '1'\n",
    "\n",
    "                    gp_zeros = list(states)\n",
    "                    gp_zeros[j] = '0'\n",
    "\n",
    "                    expanded.add(tuple(gp_zeros))\n",
    "                    expanded.add(tuple(gp_ones))\n",
    "                    \n",
    "                    expanded = expand_wildcards(expanded)\n",
    "    return expanded\n",
    "            \n",
    "\n",
    "# we will add a \"column\" for coverage on each of the transitions\n",
    "ts_coverage = []\n",
    "for transition, inputs, permutables in ts_transitions:\n",
    "    input_array = np.array([char for char in inputs])\n",
    "    # entries can have multiple group-invariant enputs\n",
    "    for subset in permutables:\n",
    "        # need numpy arrays for indexing\n",
    "        subset = np.array(list(subset))\n",
    "        gi = input_array[subset]\n",
    "        \n",
    "        # we want only permutations that are meaningfully different\n",
    "        gi_perm = np.unique(np.array(list(permutations(gi))), axis=0)\n",
    "        expansions = list(gi_perm)\n",
    "\n",
    "        # need to expand wildcards into zeros and ones\n",
    "        expansions = expand_wildcards(expansions)\n",
    "\n",
    "    # all that we really care about is how much of the LUT is covered by the schemata\n",
    "    lut_coverage = len(expansions)\n",
    "    ts_coverage.append((transition, inputs, permutables, lut_coverage))\n",
    "ts_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose the entries with wildcards into different permutable groups. In this case the entry '002' can be thought of as all permutable pairs and a wildcard. We need to preserve the literals but we can think of the wildcard as just extra shit (redundancy even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'(0,)': [],\n  '(1,)': [],\n  '(2,)': [],\n  '(0, 1)': [],\n  '(0, 2)': [],\n  '(1, 2)': [],\n  '(0, 1, 2)': [('1', '000', 1), ('1', '110', 1)]},\n {'(0,)': [],\n  '(1,)': [],\n  '(2,)': [('1', '221', 4)],\n  '(0, 1)': [('1', '012', 2)],\n  '(0, 2)': [],\n  '(1, 2)': [],\n  '(0, 1, 2)': []}]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "def literal_distribution(coverage_list):\n",
    "    \"\"\" Function that assigns need-to-know literals to subsets of inputs pretty sure its really poorly written \"\"\"\n",
    "    # set up a dictionary object that will store these literals \n",
    "    # we get the keys by using the length of the input string\n",
    "    # if for some godforsaken reason these are different lengths\n",
    "    # you have bigger problems\n",
    "    distributed = [{str(k): [] for k in powerset(range(len(coverage_list[0][1])))} for _ in range(2)]\n",
    "    for transition, inputs, pos_free, coverage in coverage_list:\n",
    "        # we have a choice to make here: if you have a permutable group of literals\n",
    "        # we have been calling that synergy. If the remaining inputs are also\n",
    "        # literal than does that mean its all synergy between them even though\n",
    "        # they are not all permutable? There is no reason to think that it is\n",
    "        # going to be redundancy because neither one are sufficient to determine\n",
    "        # the transision on their own. I think we should call it synergy.\n",
    "        input_set = set(range(len(inputs)))\n",
    "        if '2' in inputs:\n",
    "            # we need to figure out which inputs are NOT part of any permutable groups\n",
    "            for group in pos_free:\n",
    "                twos_idx = []\n",
    "                outgroup_vals = []\n",
    "                # find twos and figure out what is in the outgroup\n",
    "                outgroup = input_set - group\n",
    "                for i, char in enumerate(inputs):\n",
    "                    if i in group and char =='2':\n",
    "                        twos_idx.append(i)\n",
    "                    elif i in outgroup:\n",
    "                        outgroup_vals.append(char)\n",
    "                # collect all of the literals, if there are some in the outgroup we need to\n",
    "                # deal with them\n",
    "                outgroup_literals = []\n",
    "                if '0' in outgroup_vals or '1' in outgroup_vals:\n",
    "                    for o in outgroup:\n",
    "                        if inputs[o] != '2':\n",
    "                            outgroup_literals.append(o)\n",
    "                    \n",
    "                # distribute the literals in the dictionary\n",
    "                subgroup_size = len(group) - len(twos_idx)\n",
    "                if subgroup_size > 0:\n",
    "                    subgroups = combinations(group, len(group) - len(twos_idx))\n",
    "                    for sub in subgroups:\n",
    "                        if len(outgroup_literals) > 0:\n",
    "                            expanded_sub = tuple(list(sub).extend(outgroup_literals))\n",
    "                        else:\n",
    "                            expanded_sub = sub\n",
    "                        distributed[int(transition)][str(sub)].append((output, inputs, coverage))\n",
    "                elif len(outgroup_literals) > 0:\n",
    "                    sub = tuple(outgroup_literals)\n",
    "                    distributed[int(transition)][str(sub)].append((output, inputs, coverage))\n",
    "        # no wildcards\n",
    "        else:\n",
    "            sub = tuple(range(len(inputs)))\n",
    "            distributed[int(transition)][str(sub)].append((output, inputs, coverage))\n",
    "\n",
    "    \n",
    "    return distributed\n",
    "dist = literal_distribution(ts_coverage)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can relate these literals to the partial information lattice, or at least the sets contained therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this gets the sets as described by Williams and Beer\n",
    "def PID_sets(k):\n",
    "    \"\"\" This function returns a list of the sets in the redundancy lattice \"\"\"\n",
    "    double_powerset = list(chain.from_iterable([combinations(powerset(range(k)), r) for r in range(1, gate.k)]))\n",
    "    keep_sets = []\n",
    "    for subset in double_powerset:\n",
    "        contains_subset = False\n",
    "        for i in subset:\n",
    "            for j in subset:\n",
    "                if i != j and set(i).issubset(j):\n",
    "                    contains_subset = True\n",
    "                    break\n",
    "            if contains_subset:\n",
    "                break\n",
    "        if not contains_subset:     \n",
    "            keep_sets.append(subset)\n",
    "    return keep_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find transitions that are shared by more than one GIE. These indicate redundancy between synergies. and will populate our information terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'((0,),)': 0, '((1,),)': 0, '((2,),)': 0, '((0, 1),)': 0, '((0, 2),)': 0, '((1, 2),)': 0, '((0, 1, 2),)': 0, '((0,), (1,))': 0, '((0,), (2,))': 0, '((0,), (1, 2))': 0, '((1,), (2,))': 0, '((1,), (0, 2))': 0, '((2,), (0, 1))': 0, '((0, 1), (0, 2))': 0, '((0, 1), (1, 2))': 0, '((0, 2), (1, 2))': 0}\n[{'(0,)': 0, '(1,)': 0, '(2,)': 0, '(0, 1)': 0, '(0, 2)': 0, '(1, 2)': 0, '(0, 1, 2)': 0}, {'(0,)': 0, '(1,)': 0, '(2,)': 0, '(0, 1)': 0, '(0, 2)': 0, '(1, 2)': 0, '(0, 1, 2)': 0}]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'((0,),)': 0,\n '((1,),)': 0,\n '((2,),)': 0,\n '((0, 1),)': 0,\n '((0, 2),)': 0,\n '((1, 2),)': 0,\n '((0, 1, 2),)': 0,\n '((0,), (1,))': 0,\n '((0,), (2,))': 0,\n '((0,), (1, 2))': 0,\n '((1,), (2,))': 0,\n '((1,), (0, 2))': 0,\n '((2,), (0, 1))': 0.6084585933443496,\n '((0, 1), (0, 2))': 0,\n '((0, 1), (1, 2))': 0,\n '((0, 2), (1, 2))': 0,\n '((0, 1, 2))': 0.2028195311147832}"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "from dit.shannon.shannon import entropy\n",
    "from dit import Distribution \n",
    "# info_sets = {}\n",
    "# decomposition = {}\n",
    "# for atom in info_sets:\n",
    "#     # if there are distributed literals we can calculate this thing\n",
    "#     if len(info_sets[atom]) > 0:\n",
    "#         output_dist = Distribution(gate.outputs, [1/2**gate.k]*2**gate.k)\n",
    "#         coverage = sum([t[-1] for t in info_sets[atom]])\n",
    "#         norm_coverage = coverage / 2**gate.k * entropy(output_dist)\n",
    "#     else:\n",
    "#         norm_coverage = 0\n",
    "    \n",
    "#     decomposition[atom] = norm_coverage\n",
    "\n",
    "# decomposition\n",
    "\n",
    "for subset in keep_sets:\n",
    "    info_sets[str(subset)] = []\n",
    "\n",
    "def assign_information(distributed, gate):\n",
    "    \"\"\" this function takes the distributed literals group and assigns them to the PID sets \"\"\"\n",
    "    # set up dictionary to store these values\n",
    "    keep_sets = PID_sets(gate.k)\n",
    "    info_sets = {str(k): 0 for k in keep_sets}\n",
    "    print(info_sets)\n",
    "\n",
    "    # set up dictionary for intermediate values\n",
    "    grouped_coverage = [{str(k): 0 for k in powerset(range(gate.k))} for _ in range(2)]\n",
    "    print(grouped_coverage)\n",
    "\n",
    "    # calculate entropy of the output distribution for normalization\n",
    "    output_dist = Distribution(gate.outputs, [1/2**gate.k]*2**gate.k)\n",
    "    output_entropy = entropy(output_dist)\n",
    "\n",
    "    # we can now search for redundancy between these informative groups\n",
    "    for t, transition in enumerate(distributed):\n",
    "        redundant = []\n",
    "        for group in transition:\n",
    "            if len(transition[group]) > 0:\n",
    "                for e, entry in enumerate(transition[group]):\n",
    "                    grouped_coverage[t][group] += entry[-1]\n",
    "\n",
    "    # with the coverage calculated we can find the groups, at least in this simple case\n",
    "    for t, transition in enumerate(grouped_coverage):\n",
    "        redundant = []\n",
    "        coverage = []\n",
    "        for group in transition:\n",
    "            if transition[group] > 0:\n",
    "                redundant.append(group)\n",
    "                coverage.append(transition[group])\n",
    "        \n",
    "        # finally we can add the normalized coverage to the information dict\n",
    "        info_key = '(' + ', '.join(redundant) + ')'\n",
    "        normed_info = np.sum(coverage) / 2**gate.k * output_entropy\n",
    "        info_sets[info_key] = normed_info\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    return info_sets\n",
    "assign_information(dist, gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "I_ccs:\nUnnamed: 0 190\nrule 190\n((0,), (1,), (2,)) 3.597633285699193e-09\n((0,), (1,)) -3.5976320642641233e-09\n((0,), (2,)) -3.597633285699193e-09\n((0,), (1, 2)) 3.5976320642641233e-09\n((0,),) 0.0\n((1,), (2,)) -3.597633285699193e-09\n((1,), (0, 2)) 3.5976320642641233e-09\n((2,), (0, 1)) 0.10375937841734424\n((0, 1), (0, 2), (1, 2)) -3.59763206725372e-09\n((0, 1), (0, 2)) 0.0\n((1,),) 0.0\n((0, 1), (1, 2)) 0.0\n((0, 1),) 0.2075187496394219\n((2,),) 0.2075187496394219\n((0, 2), (1, 2)) -5.0387030325893534e-08\n((0, 2),) 5.0387030325893534e-08\n((1, 2),) 5.0387030325893534e-08\n((0, 1, 2),) 0.29248119997354793\n"
    }
   ],
   "source": [
    "ccs = pd.read_csv('../data/eca_decompositions/ccs_df.csv')\n",
    "print('I_ccs:')\n",
    "rule = ccs[ccs['rule'] == 190]\n",
    "for col in range(len(ccs.columns)):\n",
    "    print(ccs.columns[col], rule.iloc[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "I_min:\nUnnamed: 0 190\nrule 190\n((0,), (1,), (2,)) 0.0\n((0,), (1,)) 0.0\n((0,), (2,)) 0.0\n((0,), (1, 2)) 0.0\n((0,),) 0.0\n((1,), (2,)) 0.0\n((1,), (0, 2)) 0.0\n((2,), (0, 1)) 0.3112781244591328\n((0, 1), (0, 2), (1, 2)) 0.0\n((0, 1), (0, 2)) 0.0\n((1,),) 0.0\n((0, 1), (1, 2)) 0.0\n((0, 1),) 0.0\n((2,),) 0.0\n((0, 2), (1, 2)) 0.0\n((0, 2),) 0.0\n((1, 2),) 0.0\n((0, 1, 2),) 0.5\n"
    }
   ],
   "source": [
    "imin = pd.read_csv('../data/eca_decompositions/imin_df.csv')\n",
    "print('I_min:')\n",
    "rule = imin[imin['rule'] == 190]\n",
    "for col in range(len(imin.columns)):\n",
    "    print(imin.columns[col], rule.iloc[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "I_pm:\nUnnamed: 0 190\nrule 190\n((0,), (1,), (2,)) 0.4575187496394218\n((0,), (1,)) -0.4575187496394218\n((0,), (2,)) 0.0\n((0,), (1, 2)) 0.0\n((0,),) 0.0\n((1,), (2,)) 0.0\n((1,), (0, 2)) 0.0\n((2,), (0, 1)) -0.14624062518028913\n((0, 1), (0, 2), (1, 2)) 0.7075187496394221\n((0, 1), (0, 2)) 0.0\n((1,),) 0.0\n((0, 1), (1, 2)) 0.0\n((0, 1),) -0.25\n((2,),) 0.0\n((0, 2), (1, 2)) -0.25\n((0, 2),) 0.0\n((1, 2),) 0.0\n((0, 1, 2),) 0.75\n"
    }
   ],
   "source": [
    "ipm = pd.read_csv('../data/eca_decompositions/pm_df.csv')\n",
    "print('I_pm:')\n",
    "rule = ipm[ipm['rule'] == 190]\n",
    "for col in range(len(ipm.columns)):\n",
    "    print(ipm.columns[col], rule.iloc[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "I_wedge:\nUnnamed: 0 151\nrule 151\n((0,), (1,), (2,)) 0.0\n((0,), (1,)) 0.0\n((0,), (2,)) 0.0\n((0,), (1, 2)) 0.0\n((0,),) 0.048794940695398685\n((1,), (2,)) 0.0\n((1,), (0, 2)) 0.0\n((2,), (0, 1)) 0.0\n((0, 1), (0, 2), (1, 2)) 0.0\n((0, 1), (0, 2)) 0.0\n((1,),) 0.048794940695398685\n((0, 1), (1, 2)) 0.0\n((0, 1),) 0.10684412153416778\n((2,),) 0.048794940695398685\n((0, 2), (1, 2)) 0.0\n((0, 2),) 0.10684412153416778\n((1, 2),) 0.10684412153416778\n((0, 1, 2),) 0.4875168162362658\n"
    }
   ],
   "source": [
    "iwedge = pd.read_csv('../data/eca_decompositions/wedge_df.csv')\n",
    "print('I_wedge:')\n",
    "rule = iwedge[iwedge['rule'] == 151]\n",
    "for col in range(len(iwedge.columns)):\n",
    "    print(iwedge.columns[col], rule.iloc[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{2}"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "gr = {0, 1}\n",
    "set(range(3)) - gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{('0', '0'), ('0', '1'), ('1', '0', '0'), ('1', '0', '1'), ('1', '1')}"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "expand_wildcards([np.array(['1', '0', '2']), np.array(['2', '1']), np.array(['0', '0'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-180306f2e5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;36m2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}